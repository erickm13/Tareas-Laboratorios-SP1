{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erickm13/Tareas-Laboratorios-SP1/blob/main/tareas/SP1_02_Demo_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEMO #02\n",
        "\n",
        "What you will learn:\n",
        "\n",
        "\n",
        "*   How to manipulate Datasets with Pandas\n",
        "*   What is Tensorflow and how to use it\n",
        "*   How to create Neural Networks for Regression and Classification Tasks\n",
        "\n"
      ],
      "metadata": {
        "id": "_LXEnfcKV8AG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pandas (Dataset Manipulation)\n",
        "\n",
        "\"*pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.*\"\n",
        "\n",
        "[Panda's Official Documentation](https://pandas.pydata.org)\n",
        "\n",
        "-----\n",
        "\n",
        "Pandas is a powerful library used to manipulate and analyze data. This library imitates R's philosophy and Sintax (both are very similar!)\n",
        "\n",
        "Pandas is powerful at **manipulating structured data**, so keep in mind this library will shine with table-looking shaped datasets.\n",
        "In few lines, you can do a lot of stuff!\n",
        "\n",
        "Advantages:\n",
        "* Provides efficient and optimized ways to manipulate data\n",
        "* Supports multiple file formats\n",
        "* Can easily merge, divide, join, filter and other type of operations in a similar fashion to SQL!\n",
        "\n",
        "Limitaitons:\n",
        "* Can only deal with structured data\n",
        "\n",
        "Recommended Resources:\n",
        "* [Official Documentation](https://pandas.pydata.org/docs/)\n",
        "* [Geeksforgeeks](https://www.geeksforgeeks.org/pandas-tutorial/)\n",
        "* [w3Schools](https://www.w3schools.com/python/pandas/default.asp)\n"
      ],
      "metadata": {
        "id": "_ehfIR--WP8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset"
      ],
      "metadata": {
        "id": "VweteaNsr_rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "All the information regarding the dataset used for this demo can be found in the following link:\n",
        "https://archive.ics.uci.edu/ml/datasets/Computer+Hardware\n",
        "'''\n",
        "\n",
        "# Getting Dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/cpu-performance/machine.data"
      ],
      "metadata": {
        "id": "Mmt-KsURr09V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For dataset manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Loading Dataset and have a glimpse about it\n",
        "column_names = ['Vendor','Model','MYCT','MMIN','MMAX',\n",
        "                'CACH', 'CHMIN', 'CHMAX', 'PRP', 'ERP']\n",
        "\n",
        "raw_dataset = pd.read_csv(\"machine.data\", names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\",\", skipinitialspace=True)"
      ],
      "metadata": {
        "id": "sv3boXiQsDes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .describe()"
      ],
      "metadata": {
        "id": "XSJVNnNgsGDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Brief Statistical Summary of the dataset\n",
        "raw_dataset.describe()"
      ],
      "metadata": {
        "id": "K32sABzOsPZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .head() and .tail()"
      ],
      "metadata": {
        "id": "AsADBMgXsNHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of the dataset (takes the top N rows)\n",
        "raw_dataset.head(n=10)"
      ],
      "metadata": {
        "id": "0KKpcKkUsirm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of the dataset (takes the bottom N rows)\n",
        "raw_dataset.tail(n=5)"
      ],
      "metadata": {
        "id": "iuNRuBBxssmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shape"
      ],
      "metadata": {
        "id": "XFX2Y26B1akG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape is a fancy way of calling Dataset's dimension\n",
        "raw_dataset.shape"
      ],
      "metadata": {
        "id": "xwqNcqop1bjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Column Manipulation"
      ],
      "metadata": {
        "id": "sW0AAh8xs8Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Column"
      ],
      "metadata": {
        "id": "NiBaGrAgtdkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Like a dictionary, pass the EXACT name of the column to extract one column\n",
        "raw_dataset[\"Vendor\"]"
      ],
      "metadata": {
        "id": "DQg_GO4ytV_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add column\n",
        "\n"
      ],
      "metadata": {
        "id": "wvyLvhHTylm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To extract multiple, use an array instead\n",
        "raw_dataset[\"One\"] = 1\n",
        "raw_dataset"
      ],
      "metadata": {
        "id": "xTo6Bgetu7uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete Column\n"
      ],
      "metadata": {
        "id": "pJuv1IBszUEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset.pop(\"One\")\n",
        "raw_dataset"
      ],
      "metadata": {
        "id": "dFrAEXs1zV6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Column Values"
      ],
      "metadata": {
        "id": "mzqeXJmuvxgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check all the unique values\n",
        "raw_dataset[\"Vendor\"].unique()"
      ],
      "metadata": {
        "id": "JAHu_eP1v27m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the values of the column\n",
        "raw_dataset[\"Vendor\"].value_counts()"
      ],
      "metadata": {
        "id": "t0YA5BglwHCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Row Manipulation"
      ],
      "metadata": {
        "id": "QOElaoWcwPGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### \"SELECT\" and \"WHERE\""
      ],
      "metadata": {
        "id": "lascQMhSwRjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select rows that meet a column value criteria\n",
        "raw_dataset.loc[raw_dataset[\"Vendor\"] == \"ibm\"]"
      ],
      "metadata": {
        "id": "9Rgk_ZhdwwOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select rows that meet a column value criteria\n",
        "raw_dataset.loc[raw_dataset[\"MYCT\"] == 25]"
      ],
      "metadata": {
        "id": "ZzMX5FFUxJZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine conditions\n",
        "raw_dataset.loc[(raw_dataset[\"MYCT\"] <= 25) & (raw_dataset[\"MYCT\"] >= 1)]"
      ],
      "metadata": {
        "id": "hQkqpza3xbAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine conditions\n",
        "raw_dataset.loc[(raw_dataset[\"Vendor\"] == \"ibm\") & (raw_dataset[\"MYCT\"] <= 25)]"
      ],
      "metadata": {
        "id": "yI8RkTSVx7f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Subset"
      ],
      "metadata": {
        "id": "NEsdOKFfz29G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffles the rows and then returns the fraction desired\n",
        "raw_dataset.sample(frac = 0.5, random_state = 1000)"
      ],
      "metadata": {
        "id": "9o2iNrqez6mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge"
      ],
      "metadata": {
        "id": "WZSmCTn50X1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets make two dataset dummies\n",
        "dataset_one = raw_dataset.sample(frac = 0.5, random_state = 100)\n",
        "dataset_two = raw_dataset.sample(frac = 0.5, random_state = 250)"
      ],
      "metadata": {
        "id": "nVBzJvdL0Zq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No Information Loss Merge\n",
        "pd.concat([dataset_one, dataset_two])"
      ],
      "metadata": {
        "id": "UNklq7yV1Fzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JOIN merge\n",
        "pd.concat([dataset_one, dataset_two], join=\"inner\")"
      ],
      "metadata": {
        "id": "pC3BH4Lu1luE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Practices\n",
        "\n",
        "* Before loading a dataset, check the README or any file that contains information about it\n",
        "* When you are modifying a dataset, don't reuse the same variable → create a new one\n",
        "* Use descriptive names for variables → will save you a lot of hassle"
      ],
      "metadata": {
        "id": "Qh0T6-s-kl1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity\n",
        "\n",
        "* Download the [Balance Scale Dataset](https://archive.ics.uci.edu/ml/datasets/Balance+Scale)\n",
        "* Do a preview with by using .describe() and either head() or tail()\n",
        "* Check its shape\n",
        "* Remove the column \"Class Name\"\n",
        "* Create two random Subsets of the modified dataset (both with a fraction of 65%)\n",
        "* Perform an Inner Join and check the resulting shape\n"
      ],
      "metadata": {
        "id": "4m_81xTKlrcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "!wget https://archive.ics.uci.edu/static/public/12/balance+scale.zip"
      ],
      "metadata": {
        "id": "VxM5CwKxluFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/balance+scale.zip"
      ],
      "metadata": {
        "id": "eywmAJ3I1IQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For dataset manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Loading Dataset and have a glimpse about it\n",
        "column_names = ['right-distance','right-weight','left-distance','left-weight','class']\n",
        "\n",
        "raw_dataset = pd.read_csv(\"balance-scale.data\", names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\",\", skipinitialspace=True)"
      ],
      "metadata": {
        "id": "gUxpm5Co1Puu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset.describe()"
      ],
      "metadata": {
        "id": "jtj6FBkA1hRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset.head()"
      ],
      "metadata": {
        "id": "5YXVdJaM1oN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset.tail()"
      ],
      "metadata": {
        "id": "LXlJjY6F1rBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset.shape()"
      ],
      "metadata": {
        "id": "HcW-12iv1t7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_one = raw_dataset.sample(frac = 0.5, random_state = 100)\n",
        "dataset_two = raw_dataset.sample(frac = 0.5, random_state = 250)"
      ],
      "metadata": {
        "id": "OOw8zwCP1xOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No Information Loss Merge\n",
        "pd.concat([dataset_one, dataset_two])"
      ],
      "metadata": {
        "id": "MSnpCSGq14U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JOIN merge\n",
        "pd.concat([dataset_one, dataset_two], join=\"inner\")"
      ],
      "metadata": {
        "id": "aNqu3-Cc16G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Tensorflow](https://www.tensorflow.org/?hl=es-419) (AI library)\n",
        "\n",
        "TL;DR: A platform and library to work on AI related projects. It is user friendly and is easy to learn.\n",
        "\n",
        "Advantages:\n",
        "* Allows to train any kind of AI: from the simplest one to the craziest ones\n",
        "* It has Web, PC and edge technology dispositives (such as phones and microcontorllers) coverage\n",
        "* Widely used and well documented\n",
        "* Available for Python and JavaScript\n",
        "* Posseses great abstraction capabilities → User only cares about creating and technical details are hidden from them\n",
        "\n",
        "Tensorflow works with the philosophy of \"tensors flowing\" or vectors moving from one point to another.\n",
        "The library helps to develop dataflow graphs that describe how data traverses through a multidimensional graph, composed by nodes.\n",
        "\n"
      ],
      "metadata": {
        "id": "uQU0Vn6aYV3o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2hOe4K0Hmut"
      },
      "source": [
        "# 0) Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEWUjG0IIJkk"
      },
      "source": [
        "# ----- Libraries ----- #\n",
        "\n",
        "# This is the main Library that allows us to work with Neural Networks\n",
        "import tensorflow as tf\n",
        "\n",
        "# For graph plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.math import confusion_matrix\n",
        "\n",
        "# For dataset manipulation\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# For visualizing more complex graphs\n",
        "import seaborn as sns\n",
        "\n",
        "# Global constant for training acceleration\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE_cmElZJ4PE"
      },
      "source": [
        "# Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARASHphaKU6D"
      },
      "source": [
        "## 1) Dataset Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KMmvqGMGOXu"
      },
      "source": [
        "'''\n",
        "All the information regarding the dataset used for this demo can be found in the following link:\n",
        "https://archive.ics.uci.edu/ml/datasets/Computer+Hardware\n",
        "'''\n",
        "\n",
        "# Getting Dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/cpu-performance/machine.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgXcpexoIG9P",
        "collapsed": true
      },
      "source": [
        "# Loading Dataset and have a glimpse about it\n",
        "column_names = ['Vendor','Model','MYCT','MMIN','MMAX',\n",
        "                'CACH', 'CHMIN', 'CHMAX', 'PRP', 'ERP']\n",
        "\n",
        "raw_dataset = pd.read_csv(\"machine.data\", names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\",\", skipinitialspace=True)\n",
        "\n",
        "# Brief Statistical Summary of the dataset\n",
        "raw_dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W3AsBCPQMHu"
      },
      "source": [
        "# Lets check columns\n",
        "raw_dataset.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsYYb14mKiz6"
      },
      "source": [
        "# Summary of the dataset\n",
        "raw_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc8kp61PKzl5"
      },
      "source": [
        "# Returns a form of (# rows, # columns)\n",
        "raw_dataset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taL7hSd9LHpj"
      },
      "source": [
        "# Lets make a copy\n",
        "new_dataset = raw_dataset.copy()\n",
        "\n",
        "# Lets check for null values\n",
        "print(new_dataset.isna().sum())\n",
        "\n",
        "# Dropping null rows\n",
        "new_dataset = new_dataset.dropna()\n",
        "\n",
        "# Checking new dataset\n",
        "new_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-wSRqvEPoLo"
      },
      "source": [
        "# Lets visualize the data\n",
        "sns.pairplot(new_dataset[['MYCT','MMIN','MMAX',\n",
        "                'CACH', 'CHMIN', 'CHMAX', 'PRP']], diag_kind=\"kde\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKsREMcO91a"
      },
      "source": [
        "## 2) NN for a simple Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xolCD5p4QFa8"
      },
      "source": [
        "# Splitting dataset into training and testing\n",
        "train, test = train_test_split(new_dataset, test_size=0.2)\n",
        "\n",
        "# Sepparating both sets into dependent and independent variables\n",
        "independent_variables = ['MYCT','MMIN','MMAX','CACH', 'CHMIN', 'CHMAX', 'PRP']\n",
        "dependent_variables = ['ERP']\n",
        "\n",
        "train_set = train[independent_variables]\n",
        "train_target = train[dependent_variables]\n",
        "\n",
        "test_set = test[independent_variables]\n",
        "test_target = test[dependent_variables]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set"
      ],
      "metadata": {
        "id": "XqadtySjOXtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_target"
      ],
      "metadata": {
        "id": "h1eEAOkqOan-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efOETJ7cPkMO"
      },
      "source": [
        "# Lets build a simple model. NOTE: this is the construction of the architecture of the model!\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(len(independent_variables))),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "\n",
        "  # Last layer of the model and its activation function decide if it is a regression or classification problem!\n",
        "  tf.keras.layers.Dense(units=len(dependent_variables), activation='relu'),\n",
        "  ])\n",
        "\n",
        "# Now lets compile the model. NOTE: These are the finishing touches before having a fully functional model\n",
        "model.compile(\n",
        "    loss='mse',\n",
        "    optimizer='adam',\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG1tt040L5d0"
      },
      "source": [
        "# Now lets train the model!\n",
        "model.fit(train_set,\n",
        "          train_target,\n",
        "          epochs=15,\n",
        "          batch_size = 32,\n",
        "          validation_split=0.2,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBGnFIGtJ_9e"
      },
      "source": [
        "# Lets evaluate our model\n",
        "model.evaluate(x=test_set, y=test_target, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l31mWppGPKRf"
      },
      "source": [
        "## 3) NN for multiple Regressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2faCY4iaPPgl"
      },
      "source": [
        "# Splitting dataset into training and testing\n",
        "train, test = train_test_split(new_dataset, test_size=0.2)\n",
        "\n",
        "# Sepparating both sets into dependent and independent variables\n",
        "independent_variables = ['MYCT','MMIN','MMAX','CACH', 'CHMIN', 'CHMAX']\n",
        "dependent_variables = ['PRP', 'ERP']\n",
        "\n",
        "train_set = train[independent_variables]\n",
        "train_target = train[dependent_variables]\n",
        "\n",
        "test_set = test[independent_variables]\n",
        "test_target = test[dependent_variables]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRBSUiPqPWDl"
      },
      "source": [
        "# Lets build the model. NOTE: this is the construction of the architecture of the model!\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(len(independent_variables))),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=len(dependent_variables), activation='relu'),\n",
        "  ])\n",
        "\n",
        "# Now lets compile the model. NOTE: These are the finishing touches before having a fully functional model\n",
        "model.compile(loss='mse', optimizer='adam', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Now lets train the model!\n",
        "model.fit(train_set,\n",
        "          train_target,\n",
        "          epochs=10,\n",
        "          batch_size = 32 ,\n",
        "          validation_split=0.2\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets evaluate our model\n",
        "model.evaluate(x=test_set, y=test_target, batch_size=128)"
      ],
      "metadata": {
        "id": "9zFFNUGwcnLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny4qW8UlP5LQ"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okEGIBRLLg1z"
      },
      "source": [
        "## 1) Dataset Preparations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "All the information regarding the dataset used for this demo can be found in the following link:\n",
        "https://archive.ics.uci.edu/ml/datasets/Iris\n",
        "'''\n",
        "\n",
        "# Getting Dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
      ],
      "metadata": {
        "id": "bG6-fqP_dg7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncn_CmxWYqL1"
      },
      "source": [
        "# Loading Dataset and have a glimpse about it\n",
        "column_names = ['sepal_length','sepal_width','petal_length','petal_width', 'class']\n",
        "\n",
        "raw_dataset = pd.read_csv(\"iris.data\", names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\",\", skipinitialspace=True)\n",
        "\n",
        "# Brief Statistical Summary of the dataset\n",
        "raw_dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I53Qd8obhFg"
      },
      "source": [
        "# Lets check columns\n",
        "raw_dataset.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2yDVbIpblVZ"
      },
      "source": [
        "# Summary of the dataset\n",
        "raw_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW8y5U8Tbnqf"
      },
      "source": [
        "# Returns a form of (# rows, # columns)\n",
        "raw_dataset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19wGecl7bpJd"
      },
      "source": [
        "# Lets make a copy\n",
        "new_dataset = raw_dataset.copy()\n",
        "\n",
        "# Lets check for null values\n",
        "# df.dropna()\n",
        "print(new_dataset.isna().sum())\n",
        "\n",
        "# Dropping null rows\n",
        "new_dataset = new_dataset.dropna()\n",
        "\n",
        "# Checking new dataset\n",
        "new_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SwagrZYbpp2"
      },
      "source": [
        "# Lets visualize the data\n",
        "sns.pairplot(new_dataset[['sepal_length','sepal_width','petal_length','petal_width']], diag_kind=\"kde\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5QchVy_bq_t"
      },
      "source": [
        "# Splitting dataset into training and testing\n",
        "train, test = train_test_split(new_dataset, test_size=0.2)\n",
        "\n",
        "# Sepparating both sets into dependent and independent variables\n",
        "independent_variables = list(raw_dataset.columns)\n",
        "independent_variables.remove('class')\n",
        "dependent_variables = ['class']\n",
        "\n",
        "train_set = train[independent_variables]\n",
        "train_target = train[dependent_variables]\n",
        "\n",
        "test_set = test[independent_variables]\n",
        "test_target = test[dependent_variables]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_target, test_target"
      ],
      "metadata": {
        "id": "0ZbOA7bGfrNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NNs can't understand strings, we have to ENCODE them! (more next class)\n",
        "train_target = pd.factorize(train_target['class'])[0]\n",
        "test_target = pd.factorize(test_target['class'])[0]"
      ],
      "metadata": {
        "id": "Piu9aiz9fp0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_target, test_target"
      ],
      "metadata": {
        "id": "fLsO0AOnKYzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9NmOn_vecR3"
      },
      "source": [
        "## 2) NN for Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17v9Qk4xcIPi"
      },
      "source": [
        "# Lets build the model. NOTE: this is the construction of the architecture of the model!\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(len(independent_variables))),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=3, activation='softmax')\n",
        "  ])\n",
        "\n",
        "# Now lets compile the model. NOTE: These are the finishing touches before having a fully functional model\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Now lets train the model!\n",
        "model.fit(train_set,\n",
        "          train_target,\n",
        "          epochs=25,\n",
        "          batch_size = 128,\n",
        "          validation_split=0.2\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets evaluate our model\n",
        "model.evaluate(x=test_set, y=test_target, batch_size=128)"
      ],
      "metadata": {
        "id": "gTH5pRN2e_lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_target"
      ],
      "metadata": {
        "id": "uu0j1s6BMUkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aTmhnXieaFe"
      },
      "source": [
        "# Function that plots confusion matrix\n",
        "def plot_confusion_matrix(labels, predictions):\n",
        "  figure = plt.figure(figsize=(4, 4))\n",
        "  sns.heatmap(confusion_matrix(labels=labels, predictions=predictions), annot=True,cmap=plt.cm.Blues)\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "# Confusion Matrix\n",
        "predictions = list(map(lambda x: np.argmax(x), model.predict(test_set)))\n",
        "\n",
        "plot_confusion_matrix(labels=test_target, predictions=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s3DicqEjZAu"
      },
      "source": [
        "# Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO7kBZ-MPLaP"
      },
      "source": [
        "## 1) Data Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAXYOI9RZxxM"
      },
      "source": [
        "import itertools\n",
        "from keras.preprocessing import image\n",
        "\n",
        "IMG_HEIGHT = 512\n",
        "IMG_WIDTH = 256\n",
        "\n",
        "img_rows = [(i-(IMG_WIDTH/2))/(IMG_WIDTH/2) for i in range(IMG_WIDTH)]\n",
        "img_cols = [(j-(IMG_HEIGHT/2))/(IMG_HEIGHT/2) for j in range(IMG_HEIGHT)]\n",
        "\n",
        "flatten_image = np.array(list(itertools.product(img_rows, img_cols)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht0MNfvfr5cQ"
      },
      "source": [
        "## 2) NN for Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ7hMmVEr8LU"
      },
      "source": [
        "# Creating a custom Layer\n",
        "class ScaleLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, redScale=255.0, greenScale=255.0, blueScale=255.0):\n",
        "    super(ScaleLayer, self).__init__()\n",
        "    self.scale = tf.constant([redScale, greenScale, blueScale], dtype=tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    outputs = tf.dtypes.cast(inputs, tf.float32)\n",
        "    outputs = outputs * self.scale\n",
        "    return tf.dtypes.cast(outputs, tf.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2BrfvIysFzD"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(2, )),\n",
        "  tf.keras.layers.Dense(units=128, activation='tanh', kernel_constraint=tf.keras.constraints.MaxNorm(max_value=4), kernel_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4)),\n",
        "  tf.keras.layers.Dense(units=256, activation='tanh', kernel_constraint=tf.keras.constraints.MaxNorm(max_value=4), kernel_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4)),\n",
        "  tf.keras.layers.Dense(units=512, activation='tanh', kernel_constraint=tf.keras.constraints.MaxNorm(max_value=4), kernel_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4)),\n",
        "  tf.keras.layers.Dense(units=3, activation='sigmoid', kernel_initializer=tf.keras.initializers.random_normal()),\n",
        "  ScaleLayer(redScale=0, greenScale=255, blueScale=0)\n",
        "  ])\n",
        "\n",
        "generated_image = np.reshape(np.array(model(flatten_image)), newshape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(image.array_to_img(generated_image))\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}